# ScavaDados - Async Data Scraper API

This project is a complete backend service built as a technical portfolio piece. It consists of an asynchronous API built with FastAPI that scrapes book data from `books.toscrape.com`, processes it, stores it in a PostgreSQL database, and exposes endpoints to interact with the data.

The entire application is containerized with Docker and includes a full suite of automated tests and a CI/CD pipeline using GitHub Actions.

## Tech Stack

This project was built to meet the specific requirements and preferred technologies of a Junior Backend Developer (Data) role, demonstrating proficiency across the entire development lifecycle.

| Category | Technology | Aligns with Requirement/Differential |
| :--- | :--- | :--- |
| **Backend** | Python 3.11 | `Python` |
| **API Framework** | FastAPI | `Desenvolvimento de APIs`, `FastAPI`, `asyncio` |
| **Database** | PostgreSQL | `Bancos de dados SQL` |
| **ORM** | SQLAlchemy (Async) | `SQLAlchemy`, `asyncio` |
| **Data Scraping** | `httpx`, `BeautifulSoup4` | `Web scraping`, `Processamento assíncrono` |
| **Containerization** | Docker, Docker Compose | `Docker e práticas de containerização` |
| **Testing** | Pytest, `TestClient` | `Desenvolvimento orientado a testes` |
| **CI/CD** | GitHub Actions | `Práticas de desenvolvimento/integração contínuos` |

## Features

* **FastAPI Backend**: A fully asynchronous API for high performance.
* **Database**: PostgreSQL managed via Docker Compose.
* **Web Scraper**: An `async` scraper module to fetch and parse book data.
* **Testing**: A complete test suite using `pytest` to ensure code quality.
* **CI Pipeline**: A GitHub Actions workflow that automatically builds and tests the project on every push to `main`.

## API Endpoints

The API documentation is automatically generated by FastAPI and is available at `http://localhost:8000/docs` once the application is running.

* `POST /scrape/`: Triggers the web scraping process. Scrapes `books.toscrape.com`, finds 20 books, and saves any new ones to the database.
* `GET /datasets/`: Lists all datasets (books) currently stored in the database.
* `POST /datasets/`: Manually creates a new dataset in the database.

## Getting Started

### Prerequisites

* [Docker](https://www.docker.com/products/docker-desktop/) installed and running.
* [Git](https://git-scm.com/) installed.

### How to Run

This project is designed to run with a single command.

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/amrc-lima/scava-dados.git](https://github.com/amrc-lima/scava-dados.git)
    cd scava-dados
    ```

2.  **Build and run the containers:**
    ```bash
    docker-compose up --build
    ```

That's it! The services will build, and the API will be available at `http://localhost:8000`.

You can now open `http://localhost:8000/docs` in your browser to interact with the API.

## Running Tests

Tests are written with `pytest` and can be run locally (requires a Python virtual environment).

1.  **Install dependencies** (in your local `venv`):
    ```bash
    pip install -r requirements.txt
    pip install pytest asgi-lifespan
    ```

2.  **Run the test suite:**
    ```bash
    pytest
    ```